#!/usr/bin/env python3
"""
Assistant AI sp√©cialis√© ENCO - Int√©gration m√©tier des embeddings
Aide les op√©rateurs et l'encadrement avec l'intelligence artificielle
"""

import os
import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from services.embeddings import (
    embedding_service, 
    semantic_search_docs, 
    categorize_document,
    get_embedding
)
from utils.firestore import db
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# Configuration OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logging.warning("‚ö†Ô∏è OPENAI_API_KEY non d√©finie - Assistant AI d√©sactiv√©")

class ENCOAIAssistant:
    """Assistant AI sp√©cialis√© pour ENCO"""
    
    def __init__(self):
        self.client = None
        if OPENAI_API_KEY and OpenAI:
            try:
                self.client = OpenAI(api_key=OPENAI_API_KEY)
                logging.info("‚úÖ Assistant AI ENCO initialis√©")
            except Exception as e:
                logging.error(f"‚ùå Erreur initialisation OpenAI: {e}")
                self.client = None
        else:
            logging.warning("‚ö†Ô∏è Assistant AI d√©sactiv√© (pas d'API key ou OpenAI non disponible)")
    
    # ===== CHATBOT FIRESTORE =====
    
    async def process_firestore_message(self, message_doc_ref, prompt: str) -> bool:
        """
        Traite un message Firestore et g√©n√®re une r√©ponse IA
        
        Args:
            message_doc_ref: R√©f√©rence du document Firestore
            prompt: Le message de l'utilisateur
            
        Returns:
            True si succ√®s, False sinon
        """
        try:
            # Mettre √† jour le statut en cours de traitement
            message_doc_ref.update({
                'status': {
                    'state': 'processing',
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
            })
            
            # G√©n√©rer la r√©ponse avec contexte ferroviaire
            response = await self.generate_railway_response(prompt)
            
            # Mettre √† jour avec la r√©ponse
            message_doc_ref.update({
                'response': response,
                'status': {
                    'state': 'completed',
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
            })
            
            logging.info(f"‚úÖ R√©ponse g√©n√©r√©e pour: {prompt[:50]}...")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur traitement message Firestore: {e}")
            try:
                message_doc_ref.update({
                    'status': {
                        'state': 'error',
                        'error': str(e),
                        'created_at': datetime.now().isoformat(),
                        'updated_at': datetime.now().isoformat()
                    }
                })
            except:
                pass
            return False
    
    async def generate_railway_response(self, prompt: str) -> str:
        """
        G√©n√®re une r√©ponse IA avec contexte ferroviaire
        
        Args:
            prompt: Message de l'utilisateur
            
        Returns:
            R√©ponse g√©n√©r√©e
        """
        if not self.client:
            return "‚ö†Ô∏è Assistant AI non disponible (OpenAI non configur√©)"
        
        try:
            # Contexte ferroviaire enrichi
            railway_context = """
            Tu es un assistant sp√©cialis√© dans le domaine ferroviaire pour ENCO.
            Tu peux aider avec :
            - Anomalies et incidents techniques
            - Maintenance pr√©ventive et curative
            - S√©curit√© et signalisation
            - Proc√©dures op√©rationnelles
            - √âquipements et mat√©riel roulant
            - R√©glementation ferroviaire
            - Optimisation des op√©rations
            
            R√©ponds de mani√®re professionnelle et technique, adapt√©e aux op√©rateurs ferroviaires.
            """
            
            # Analyser le type de demande
            prompt_lower = prompt.lower()
            
            if any(word in prompt_lower for word in ['anomalie', 'probl√®me', 'incident', 'erreur']):
                # Demande li√©e aux anomalies
                similar_anomalies = self.analyze_anomalie_similarity(prompt, days_back=30)
                if similar_anomalies:
                    context = f"\n\nAnomalies similaires r√©centes :\n"
                    for i, anomaly in enumerate(similar_anomalies[:3], 1):
                        context += f"{i}. {anomaly['description'][:100]}... (Score: {anomaly['similarity_score']:.2f})\n"
                else:
                    context = "\n\nAucune anomalie similaire trouv√©e dans l'historique r√©cent."
                
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}{context}\n\nAnalyse cette demande et fournis une r√©ponse technique appropri√©e."
            
            elif any(word in prompt_lower for word in ['maintenance', 'r√©paration', 'entretien']):
                # Demande de maintenance
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}\n\nFournis des conseils de maintenance et des proc√©dures appropri√©es."
            
            elif any(word in prompt_lower for word in ['s√©curit√©', 'signal', 'r√®glement']):
                # Demande de s√©curit√©
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}\n\nExplique les aspects de s√©curit√© et les r√©glementations applicables."
            
            else:
                # Demande g√©n√©rale
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}\n\nR√©ponds de mani√®re utile et technique."
            
            # Appel √† l'IA
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": full_prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"‚ùå Erreur g√©n√©ration r√©ponse: {e}")
            return f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}"
    
    def start_firestore_listener(self):
        """
        D√©marre l'√©couteur Firestore pour les nouveaux messages
        """
        if not db:
            logging.error("‚ùå Firestore non initialis√© - impossible de d√©marrer l'√©couteur")
            return
        
        try:
            # √âcouter tous les messages dans la collection users/{uid}/messages
            messages_ref = db.collection_group('messages')
            
            def on_snapshot(doc_snapshot, changes, read_time):
                for change in changes:
                    if change.type.name == 'ADDED':
                        doc = change.document
                        data = doc.to_dict()
                        
                        # V√©rifier si c'est un nouveau message sans r√©ponse
                        if data.get('prompt') and not data.get('response'):
                            logging.info(f"üÜï Nouveau message d√©tect√©: {data['prompt'][:50]}...")
                            
                            # Correction asynchrone robuste
                            loop = None
                            try:
                                loop = asyncio.get_running_loop()
                            except RuntimeError:
                                try:
                                    loop = asyncio.get_event_loop()
                                except Exception:
                                    loop = None
                            if loop and loop.is_running():
                                asyncio.run_coroutine_threadsafe(
                                    self.process_firestore_message(doc.reference, data['prompt']),
                                    loop
                                )
                            else:
                                asyncio.run(self.process_firestore_message(doc.reference, data['prompt']))
            
            # D√©marrer l'√©couteur
            watch = messages_ref.on_snapshot(on_snapshot)
            logging.info("‚úÖ √âcouteur Firestore d√©marr√© pour les messages")
            
            return watch
            
        except Exception as e:
            logging.error(f"‚ùå Erreur d√©marrage √©couteur Firestore: {e}")
            return None
    
    def add_test_message(self, uid: str, prompt: str) -> bool:
        """
        Ajoute un message de test dans Firestore
        
        Args:
            uid: ID de l'utilisateur
            prompt: Message √† envoyer
            
        Returns:
            True si succ√®s
        """
        if not db:
            logging.error("‚ùå Firestore non initialis√©")
            return False
        
        try:
            # Ajouter le message
            message_ref = db.collection(f'users/{uid}/messages').add({
                'prompt': prompt,
                'timestamp': datetime.now().isoformat()
            })
            
            logging.info(f"‚úÖ Message de test ajout√©: {prompt[:50]}...")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå Erreur ajout message test: {e}")
            return False

    # ===== ANALYSE D'ANOMALIES =====
    
    def analyze_anomalie_similarity(self, new_anomalie_description: str, days_back: int = 30) -> List[Dict]:
        """
        Trouve les anomalies similaires dans l'historique r√©cent
        
        Args:
            new_anomalie_description: Description de la nouvelle anomalie
            days_back: Nombre de jours √† analyser en arri√®re
            
        Returns:
            Liste des anomalies similaires avec scores
        """
        if not self.client or not db:
            return []
        
        try:
            # R√©cup√©rer les anomalies r√©centes
            cutoff_date = datetime.now() - timedelta(days=days_back)
            anomalies_ref = db.collection('anomalies')
            anomalies = anomalies_ref.where('timestamp', '>=', cutoff_date.isoformat()).stream()
            
            # Extraire les descriptions
            anomaly_descriptions = []
            anomaly_data = []
            
            for anomaly in anomalies:
                desc = anomaly.to_dict().get('description', '')
                if desc:
                    anomaly_descriptions.append(desc)
                    anomaly_data.append(anomaly.to_dict())
            
            if not anomaly_descriptions:
                return []
            
            # Recherche s√©mantique
            results = semantic_search_docs(new_anomalie_description, anomaly_descriptions, top_k=5)
            
            # Formater les r√©sultats
            similar_anomalies = []
            for doc_index, score in results:
                if score > 0.6:  # Seuil de similarit√©
                    anomaly = anomaly_data[doc_index]
                    similar_anomalies.append({
                        'description': anomaly.get('description', ''),
                        'machine': anomaly.get('machine', ''),
                        'type_anomalie': anomaly.get('type_anomalie', ''),
                        'timestamp': anomaly.get('timestamp', ''),
                        'operator': anomaly.get('operatorName', ''),
                        'similarity_score': score,
                        'resolution': anomaly.get('resolution', ''),
                        'status': anomaly.get('handled', False)
                    })
            
            return similar_anomalies
            
        except Exception as e:
            logging.error(f"‚ùå Erreur analyse similarit√© anomalie: {e}")
            return []
    
    def suggest_anomalie_resolution(self, anomalie_description: str) -> Optional[str]:
        """
        Sugg√®re une r√©solution bas√©e sur l'historique des anomalies
        
        Args:
            anomalie_description: Description de l'anomalie
            
        Returns:
            Suggestion de r√©solution ou None
        """
        if not self.client:
            return None
        
        try:
            # Trouver des anomalies similaires r√©solues
            similar_anomalies = self.analyze_anomalie_similarity(anomalie_description, days_back=90)
            resolved_anomalies = [a for a in similar_anomalies if a.get('status') and a.get('resolution')]
            
            if not resolved_anomalies:
                return None
            
            # Cr√©er un prompt pour l'IA
            prompt = f"""
            En tant qu'expert ferroviaire, analyse cette anomalie et sugg√®re une r√©solution bas√©e sur l'historique :

            NOUVELLE ANOMALIE : {anomalie_description}

            ANOMALIES SIMILAIRES R√âSOLUES :
            """
            
            for i, anomaly in enumerate(resolved_anomalies[:3], 1):
                prompt += f"""
                {i}. Description : {anomaly['description']}
                   Machine : {anomaly['machine']}
                   R√©solution appliqu√©e : {anomaly['resolution']}
                   Score de similarit√© : {anomaly['similarity_score']:.2f}
                """
            
            prompt += """
            Sugg√®re une r√©solution courte et pratique pour la nouvelle anomalie, bas√©e sur les solutions qui ont fonctionn√©.
            R√©ponse en 2-3 phrases maximum.
            """
            
            # Appel √† l'IA
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"‚ùå Erreur suggestion r√©solution: {e}")
            return None
    
    # ===== CAT√âGORISATION INTELLIGENTE =====
    
    def categorize_anomalie(self, description: str) -> Optional[str]:
        """
        Cat√©gorise automatiquement une anomalie
        
        Args:
            description: Description de l'anomalie
            
        Returns:
            Cat√©gorie sugg√©r√©e ou None
        """
        categories = [
            "Infrastructure voie ferr√©e",
            "Signalisation et s√©curit√©",
            "√âquipements de traction",
            "Syst√®mes √©lectriques",
            "Maintenance pr√©ventive",
            "S√©curit√© des passages",
            "√âquipements de communication",
            "Probl√®mes environnementaux"
        ]
        
        return categorize_document(description, categories)
    
    def prioritize_urgence(self, description: str) -> Dict[str, Any]:
        """
        √âvalue la priorit√© d'une urgence bas√©e sur son contenu
        
        Args:
            description: Description de l'urgence
            
        Returns:
            Dict avec priorit√© et justification
        """
        if not self.client:
            return {"priority": "medium", "reason": "Assistant AI non disponible"}
        
        try:
            # Mots-cl√©s critiques
            critical_keywords = [
                "d√©raillement", "collision", "incendie", "explosion", "personne bless√©e",
                "voie coup√©e", "signalisation d√©faillante", "freinage d'urgence"
            ]
            
            # V√©rifier les mots-cl√©s critiques
            desc_lower = description.lower()
            critical_found = [kw for kw in critical_keywords if kw in desc_lower]
            
            if critical_found:
                return {
                    "priority": "critical",
                    "reason": f"Mot-cl√© critique d√©tect√© : {', '.join(critical_found)}",
                    "immediate_action": True
                }
            
            # Analyse s√©mantique avec l'IA
            prompt = f"""
            √âvalue la priorit√© de cette urgence ferroviaire :
            
            DESCRIPTION : {description}
            
            R√©ponds uniquement avec :
            - PRIORITY: high/medium/low
            - REASON: justification courte
            - IMMEDIATE_ACTION: true/false
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # Parser la r√©ponse
            priority = "medium"
            reason = "Analyse automatique"
            immediate_action = False
            
            for line in result_text.split('\n'):
                if line.startswith('PRIORITY:'):
                    priority = line.split(':')[1].strip().lower()
                elif line.startswith('REASON:'):
                    reason = line.split(':')[1].strip()
                elif line.startswith('IMMEDIATE_ACTION:'):
                    immediate_action = line.split(':')[1].strip().lower() == 'true'
            
            return {
                "priority": priority,
                "reason": reason,
                "immediate_action": immediate_action
            }
            
        except Exception as e:
            logging.error(f"‚ùå Erreur √©valuation priorit√©: {e}")
            return {"priority": "medium", "reason": "Erreur d'analyse"}
    
    # ===== ASSISTANT OP√âRATEUR =====
    
    def get_operator_assistance(self, question: str, operator_context: Dict = None) -> Optional[str]:
        """
        Assistant pour les op√©rateurs - r√©pond aux questions techniques
        
        Args:
            question: Question de l'op√©rateur
            operator_context: Contexte de l'op√©rateur (position, machine, etc.)
            
        Returns:
            R√©ponse d'assistance ou None
        """
        if not self.client:
            return None
        
        try:
            # Contexte de base
            context = f"""
            Tu es un assistant technique sp√©cialis√© dans le domaine ferroviaire pour ENCO.
            Tu aides les op√©rateurs sur le terrain avec des conseils pratiques et techniques.
            """
            
            if operator_context:
                context += f"""
                Contexte op√©rateur :
                - Machine : {operator_context.get('machine', 'Non sp√©cifi√©e')}
                - Position : {operator_context.get('location', 'Non sp√©cifi√©e')}
                - Type d'activit√© : {operator_context.get('activity_type', 'Non sp√©cifi√©e')}
                """
            
            prompt = f"""
            {context}
            
            QUESTION OP√âRATEUR : {question}
            
            R√©ponds de mani√®re claire, concise et pratique. 
            Si tu ne sais pas, dis-le honn√™tement.
            R√©ponse en 2-3 phrases maximum.
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"‚ùå Erreur assistance op√©rateur: {e}")
            return None
    
    # ===== ANALYSE DE TENDANCES =====
    
    def analyze_trends(self, days_back: int = 30) -> Dict[str, Any]:
        """
        Analyse les tendances des anomalies et incidents
        
        Args:
            days_back: Nombre de jours √† analyser
            
        Returns:
            Dict avec analyses et recommandations
        """
        if not db:
            return {"error": "Base de donn√©es non disponible"}
        
        try:
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            # R√©cup√©rer les donn√©es
            anomalies = db.collection('anomalies').where('timestamp', '>=', cutoff_date.isoformat()).stream()
            urgences = db.collection('urgences').where('timestamp', '>=', cutoff_date.isoformat()).stream()
            
            # Analyser les tendances
            anomaly_descriptions = []
            urgence_descriptions = []
            
            for anomaly in anomalies:
                desc = anomaly.to_dict().get('description', '')
                if desc:
                    anomaly_descriptions.append(desc)
            
            for urgence in urgences:
                desc = urgence.to_dict().get('description', '')
                if desc:
                    urgence_descriptions.append(desc)
            
            # Cat√©goriser les anomalies
            categories = {}
            for desc in anomaly_descriptions:
                category = self.categorize_anomalie(desc)
                if category:
                    categories[category] = categories.get(category, 0) + 1
            
            # Analyser les priorit√©s d'urgence
            urgence_priorities = []
            for desc in urgence_descriptions:
                priority = self.prioritize_urgence(desc)
                urgence_priorities.append(priority['priority'])
            
            # G√©n√©rer des recommandations
            recommendations = []
            
            if categories:
                most_common = max(categories.items(), key=lambda x: x[1])
                recommendations.append(f"Cat√©gorie la plus fr√©quente : {most_common[0]} ({most_common[1]} incidents)")
            
            if urgence_priorities:
                critical_count = urgence_priorities.count('critical')
                if critical_count > 0:
                    recommendations.append(f"‚ö†Ô∏è {critical_count} urgence(s) critique(s) d√©tect√©e(s)")
            
            return {
                "period": f"{days_back} derniers jours",
                "total_anomalies": len(anomaly_descriptions),
                "total_urgences": len(urgence_descriptions),
                "categories_distribution": categories,
                "urgence_priorities": urgence_priorities,
                "recommendations": recommendations
            }
            
        except Exception as e:
            logging.error(f"‚ùå Erreur analyse tendances: {e}")
            return {"error": str(e)}

# Instance globale
ai_assistant = ENCOAIAssistant()

# Fonctions utilitaires pour faciliter l'utilisation
def analyze_similar_anomalies(description: str) -> List[Dict]:
    """Raccourci pour analyser les anomalies similaires"""
    return ai_assistant.analyze_anomalie_similarity(description)

def suggest_resolution(description: str) -> Optional[str]:
    """Raccourci pour sugg√©rer une r√©solution"""
    return ai_assistant.suggest_anomalie_resolution(description)

def get_operator_help(question: str, context: Dict = None) -> Optional[str]:
    """Raccourci pour l'assistance op√©rateur"""
    return ai_assistant.get_operator_assistance(question, context)

def analyze_system_trends(days: int = 30) -> Dict[str, Any]:
    """Raccourci pour l'analyse des tendances"""
    return ai_assistant.analyze_trends(days) 