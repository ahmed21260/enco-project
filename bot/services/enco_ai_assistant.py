#!/usr/bin/env python3
"""
Assistant AI spÃ©cialisÃ© ENCO - IntÃ©gration mÃ©tier des embeddings
Aide les opÃ©rateurs et l'encadrement avec l'intelligence artificielle
"""

import os
import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from services.embeddings import (
    embedding_service, 
    semantic_search_docs, 
    categorize_document,
    get_embedding
)
from utils.firestore import db
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# Configuration OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logging.warning("âš ï¸ OPENAI_API_KEY non dÃ©finie - Assistant AI dÃ©sactivÃ©")

class ENCOAIAssistant:
    """Assistant AI spÃ©cialisÃ© pour ENCO"""
    
    def __init__(self):
        self.client = None
        if OPENAI_API_KEY and OpenAI:
            try:
                self.client = OpenAI(api_key=OPENAI_API_KEY)
                logging.info("âœ… Assistant AI ENCO initialisÃ©")
            except Exception as e:
                logging.error(f"âŒ Erreur initialisation OpenAI: {e}")
                self.client = None
        else:
            logging.warning("âš ï¸ Assistant AI dÃ©sactivÃ© (pas d'API key ou OpenAI non disponible)")
    
    # ===== CHATBOT FIRESTORE =====
    
    async def process_firestore_message(self, message_doc_ref, prompt: str) -> bool:
        """
        Traite un message Firestore et gÃ©nÃ¨re une rÃ©ponse IA
        
        Args:
            message_doc_ref: RÃ©fÃ©rence du document Firestore
            prompt: Le message de l'utilisateur
            
        Returns:
            True si succÃ¨s, False sinon
        """
        try:
            # Mettre Ã  jour le statut en cours de traitement
            message_doc_ref.update({
                'status': {
                    'state': 'processing',
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
            })
            
            # GÃ©nÃ©rer la rÃ©ponse avec contexte ferroviaire
            response = await self.generate_railway_response(prompt)
            
            # Mettre Ã  jour avec la rÃ©ponse
            message_doc_ref.update({
                'response': response,
                'status': {
                    'state': 'completed',
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat()
                }
            })
            
            logging.info(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e pour: {prompt[:50]}...")
            return True
            
        except Exception as e:
            logging.error(f"âŒ Erreur traitement message Firestore: {e}")
            try:
                message_doc_ref.update({
                    'status': {
                        'state': 'error',
                        'error': str(e),
                        'created_at': datetime.now().isoformat(),
                        'updated_at': datetime.now().isoformat()
                    }
                })
            except:
                pass
            return False
    
    async def generate_railway_response(self, prompt: str) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse IA avec contexte ferroviaire
        
        Args:
            prompt: Message de l'utilisateur
            
        Returns:
            RÃ©ponse gÃ©nÃ©rÃ©e
        """
        if not self.client:
            return "âš ï¸ Assistant AI non disponible (OpenAI non configurÃ©)"
        
        try:
            # Contexte ferroviaire enrichi
            railway_context = """
            Tu es un assistant spÃ©cialisÃ© dans le domaine ferroviaire pour ENCO.
            Tu peux aider avec :
            - Anomalies et incidents techniques
            - Maintenance prÃ©ventive et curative
            - SÃ©curitÃ© et signalisation
            - ProcÃ©dures opÃ©rationnelles
            - Ã‰quipements et matÃ©riel roulant
            - RÃ©glementation ferroviaire
            - Optimisation des opÃ©rations
            
            RÃ©ponds de maniÃ¨re professionnelle et technique, adaptÃ©e aux opÃ©rateurs ferroviaires.
            """
            
            # Analyser le type de demande
            prompt_lower = prompt.lower()
            
            if any(word in prompt_lower for word in ['anomalie', 'problÃ¨me', 'incident', 'erreur']):
                # Demande liÃ©e aux anomalies
                similar_anomalies = self.analyze_anomalie_similarity(prompt, days_back=30)
                if similar_anomalies:
                    context = f"\n\nAnomalies similaires rÃ©centes :\n"
                    for i, anomaly in enumerate(similar_anomalies[:3], 1):
                        context += f"{i}. {anomaly['description'][:100]}... (Score: {anomaly['similarity_score']:.2f})\n"
                else:
                    context = "\n\nAucune anomalie similaire trouvÃ©e dans l'historique rÃ©cent."
                
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}{context}\n\nAnalyse cette demande et fournis une rÃ©ponse technique appropriÃ©e."
            
            elif any(word in prompt_lower for word in ['maintenance', 'rÃ©paration', 'entretien']):
                # Demande de maintenance
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}\n\nFournis des conseils de maintenance et des procÃ©dures appropriÃ©es."
            
            elif any(word in prompt_lower for word in ['sÃ©curitÃ©', 'signal', 'rÃ¨glement']):
                # Demande de sÃ©curitÃ©
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}\n\nExplique les aspects de sÃ©curitÃ© et les rÃ©glementations applicables."
            
            else:
                # Demande gÃ©nÃ©rale
                full_prompt = f"{railway_context}\n\nDemande utilisateur : {prompt}\n\nRÃ©ponds de maniÃ¨re utile et technique."
            
            # Appel Ã  l'IA
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": full_prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"âŒ Erreur gÃ©nÃ©ration rÃ©ponse: {e}")
            return f"âŒ Erreur lors de la gÃ©nÃ©ration de la rÃ©ponse : {str(e)}"
    
    def start_firestore_listener(self):
        """
        DÃ©marre l'Ã©couteur Firestore pour les nouveaux messages
        """
        if not db:
            logging.error("âŒ Firestore non initialisÃ© - impossible de dÃ©marrer l'Ã©couteur")
            return
        
        try:
            # Ã‰couter tous les messages dans la collection users/{uid}/messages
            messages_ref = db.collection_group('messages')
            
            def on_snapshot(doc_snapshot, changes, read_time):
                for change in changes:
                    if change.type.name == 'ADDED':
                        doc = change.document
                        data = doc.to_dict()
                        
                        # VÃ©rifier si c'est un nouveau message sans rÃ©ponse
                        if data.get('prompt') and not data.get('response'):
                            logging.info(f"ğŸ†• Nouveau message dÃ©tectÃ©: {data['prompt'][:50]}...")
                            
                            # Correction asynchrone robuste
                            loop = None
                            try:
                                loop = asyncio.get_running_loop()
                            except RuntimeError:
                                try:
                                    loop = asyncio.get_event_loop()
                                except Exception:
                                    loop = None
                            if loop and loop.is_running():
                                asyncio.run_coroutine_threadsafe(
                                    self.process_firestore_message(doc.reference, data['prompt']),
                                    loop
                                )
                            else:
                                asyncio.run(self.process_firestore_message(doc.reference, data['prompt']))
            
            # DÃ©marrer l'Ã©couteur
            watch = messages_ref.on_snapshot(on_snapshot)
            logging.info("âœ… Ã‰couteur Firestore dÃ©marrÃ© pour les messages")
            
            return watch
            
        except Exception as e:
            logging.error(f"âŒ Erreur dÃ©marrage Ã©couteur Firestore: {e}")
            return None
    
    def add_test_message(self, uid: str, prompt: str) -> bool:
        """
        Ajoute un message de test dans Firestore
        
        Args:
            uid: ID de l'utilisateur
            prompt: Message Ã  envoyer
            
        Returns:
            True si succÃ¨s
        """
        if not db:
            logging.error("âŒ Firestore non initialisÃ©")
            return False
        
        try:
            # Ajouter le message
            message_ref = db.collection(f'users/{uid}/messages').add({
                'prompt': prompt,
                'timestamp': datetime.now().isoformat()
            })
            
            logging.info(f"âœ… Message de test ajoutÃ©: {prompt[:50]}...")
            return True
            
        except Exception as e:
            logging.error(f"âŒ Erreur ajout message test: {e}")
            return False

    # ===== ANALYSE D'ANOMALIES =====
    
    def analyze_anomalie_similarity(self, new_anomalie_description: str, days_back: int = 30) -> List[Dict]:
        """
        Trouve les anomalies similaires dans l'historique rÃ©cent
        
        Args:
            new_anomalie_description: Description de la nouvelle anomalie
            days_back: Nombre de jours Ã  analyser en arriÃ¨re
            
        Returns:
            Liste des anomalies similaires avec scores
        """
        if not self.client or not db:
            return []
        
        try:
            # RÃ©cupÃ©rer les anomalies rÃ©centes
            cutoff_date = datetime.now() - timedelta(days=days_back)
            anomalies_ref = db.collection('anomalies')
            anomalies = anomalies_ref.where('timestamp', '>=', cutoff_date.isoformat()).stream()
            
            # Extraire les descriptions
            anomaly_descriptions = []
            anomaly_data = []
            
            for anomaly in anomalies:
                desc = anomaly.to_dict().get('description', '')
                if desc:
                    anomaly_descriptions.append(desc)
                    anomaly_data.append(anomaly.to_dict())
            
            if not anomaly_descriptions:
                return []
            
            # Recherche sÃ©mantique
            results = semantic_search_docs(new_anomalie_description, anomaly_descriptions, top_k=5)
            
            # Formater les rÃ©sultats
            similar_anomalies = []
            for doc_index, score in results:
                if score > 0.6:  # Seuil de similaritÃ©
                    anomaly = anomaly_data[doc_index]
                    similar_anomalies.append({
                        'description': anomaly.get('description', ''),
                        'machine': anomaly.get('machine', ''),
                        'type_anomalie': anomaly.get('type_anomalie', ''),
                        'timestamp': anomaly.get('timestamp', ''),
                        'operator': anomaly.get('operatorName', ''),
                        'similarity_score': score,
                        'resolution': anomaly.get('resolution', ''),
                        'status': anomaly.get('handled', False)
                    })
            
            return similar_anomalies
            
        except Exception as e:
            logging.error(f"âŒ Erreur analyse similaritÃ© anomalie: {e}")
            return []
    
    def suggest_anomalie_resolution(self, anomalie_description: str) -> Optional[str]:
        """
        SuggÃ¨re une rÃ©solution basÃ©e sur l'historique des anomalies
        
        Args:
            anomalie_description: Description de l'anomalie
            
        Returns:
            Suggestion de rÃ©solution ou None
        """
        if not self.client:
            return None
        
        try:
            # Trouver des anomalies similaires rÃ©solues
            similar_anomalies = self.analyze_anomalie_similarity(anomalie_description, days_back=90)
            resolved_anomalies = [a for a in similar_anomalies if a.get('status') and a.get('resolution')]
            
            if not resolved_anomalies:
                return None
            
            # CrÃ©er un prompt pour l'IA
            prompt = f"""
            En tant qu'expert ferroviaire, analyse cette anomalie et suggÃ¨re une rÃ©solution basÃ©e sur l'historique :

            NOUVELLE ANOMALIE : {anomalie_description}

            ANOMALIES SIMILAIRES RÃ‰SOLUES :
            """
            
            for i, anomaly in enumerate(resolved_anomalies[:3], 1):
                prompt += f"""
                {i}. Description : {anomaly['description']}
                   Machine : {anomaly['machine']}
                   RÃ©solution appliquÃ©e : {anomaly['resolution']}
                   Score de similaritÃ© : {anomaly['similarity_score']:.2f}
                """
            
            prompt += """
            SuggÃ¨re une rÃ©solution courte et pratique pour la nouvelle anomalie, basÃ©e sur les solutions qui ont fonctionnÃ©.
            RÃ©ponse en 2-3 phrases maximum.
            """
            
            # Appel Ã  l'IA
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=150,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"âŒ Erreur suggestion rÃ©solution: {e}")
            return None
    
    # ===== CATÃ‰GORISATION INTELLIGENTE =====
    
    def categorize_anomalie(self, description: str) -> Optional[str]:
        """
        CatÃ©gorise automatiquement une anomalie
        
        Args:
            description: Description de l'anomalie
            
        Returns:
            CatÃ©gorie suggÃ©rÃ©e ou None
        """
        categories = [
            "Infrastructure voie ferrÃ©e",
            "Signalisation et sÃ©curitÃ©",
            "Ã‰quipements de traction",
            "SystÃ¨mes Ã©lectriques",
            "Maintenance prÃ©ventive",
            "SÃ©curitÃ© des passages",
            "Ã‰quipements de communication",
            "ProblÃ¨mes environnementaux"
        ]
        
        return categorize_document(description, categories)
    
    def prioritize_urgence(self, description: str) -> Dict[str, Any]:
        """
        Ã‰value la prioritÃ© d'une urgence basÃ©e sur son contenu
        
        Args:
            description: Description de l'urgence
            
        Returns:
            Dict avec prioritÃ© et justification
        """
        if not self.client:
            return {"priority": "medium", "reason": "Assistant AI non disponible"}
        
        try:
            # Mots-clÃ©s critiques
            critical_keywords = [
                "dÃ©raillement", "collision", "incendie", "explosion", "personne blessÃ©e",
                "voie coupÃ©e", "signalisation dÃ©faillante", "freinage d'urgence"
            ]
            
            # VÃ©rifier les mots-clÃ©s critiques
            desc_lower = description.lower()
            critical_found = [kw for kw in critical_keywords if kw in desc_lower]
            
            if critical_found:
                return {
                    "priority": "critical",
                    "reason": f"Mot-clÃ© critique dÃ©tectÃ© : {', '.join(critical_found)}",
                    "immediate_action": True
                }
            
            # Analyse sÃ©mantique avec l'IA
            prompt = f"""
            Ã‰value la prioritÃ© de cette urgence ferroviaire :
            
            DESCRIPTION : {description}
            
            RÃ©ponds uniquement avec :
            - PRIORITY: high/medium/low
            - REASON: justification courte
            - IMMEDIATE_ACTION: true/false
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # Parser la rÃ©ponse
            priority = "medium"
            reason = "Analyse automatique"
            immediate_action = False
            
            for line in result_text.split('\n'):
                if line.startswith('PRIORITY:'):
                    priority = line.split(':')[1].strip().lower()
                elif line.startswith('REASON:'):
                    reason = line.split(':')[1].strip()
                elif line.startswith('IMMEDIATE_ACTION:'):
                    immediate_action = line.split(':')[1].strip().lower() == 'true'
            
            return {
                "priority": priority,
                "reason": reason,
                "immediate_action": immediate_action
            }
            
        except Exception as e:
            logging.error(f"âŒ Erreur Ã©valuation prioritÃ©: {e}")
            return {"priority": "medium", "reason": "Erreur d'analyse"}
    
    # ===== ASSISTANT OPÃ‰RATEUR =====
    
    def get_operator_assistance(self, question: str, operator_context: Dict = None) -> Optional[str]:
        """
        Assistant pour les opÃ©rateurs - rÃ©pond aux questions techniques
        
        Args:
            question: Question de l'opÃ©rateur
            operator_context: Contexte de l'opÃ©rateur (position, machine, etc.)
            
        Returns:
            RÃ©ponse d'assistance ou None
        """
        if not self.client:
            return None
        
        try:
            # Contexte de base
            context = f"""
            Tu es un assistant technique spÃ©cialisÃ© dans le domaine ferroviaire pour ENCO.
            Tu aides les opÃ©rateurs sur le terrain avec des conseils pratiques et techniques.
            """
            
            if operator_context:
                context += f"""
                Contexte opÃ©rateur :
                - Machine : {operator_context.get('machine', 'Non spÃ©cifiÃ©e')}
                - Position : {operator_context.get('location', 'Non spÃ©cifiÃ©e')}
                - Type d'activitÃ© : {operator_context.get('activity_type', 'Non spÃ©cifiÃ©e')}
                """
            
            prompt = f"""
            {context}
            
            QUESTION OPÃ‰RATEUR : {question}
            
            RÃ©ponds de maniÃ¨re claire, concise et pratique. 
            Si tu ne sais pas, dis-le honnÃªtement.
            RÃ©ponse en 2-3 phrases maximum.
            """
            
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=200,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logging.error(f"âŒ Erreur assistance opÃ©rateur: {e}")
            return None
    
    # ===== ANALYSE DE TENDANCES =====
    
    def analyze_trends(self, days_back: int = 30) -> Dict[str, Any]:
        """
        Analyse les tendances des anomalies et incidents
        
        Args:
            days_back: Nombre de jours Ã  analyser
            
        Returns:
            Dict avec analyses et recommandations
        """
        if not db:
            return {"error": "Base de donnÃ©es non disponible"}
        
        try:
            cutoff_date = datetime.now() - timedelta(days=days_back)
            
            # RÃ©cupÃ©rer les donnÃ©es
            anomalies = db.collection('anomalies').where('timestamp', '>=', cutoff_date.isoformat()).stream()
            urgences = db.collection('urgences').where('timestamp', '>=', cutoff_date.isoformat()).stream()
            
            # Analyser les tendances
            anomaly_descriptions = []
            urgence_descriptions = []
            
            for anomaly in anomalies:
                desc = anomaly.to_dict().get('description', '')
                if desc:
                    anomaly_descriptions.append(desc)
            
            for urgence in urgences:
                desc = urgence.to_dict().get('description', '')
                if desc:
                    urgence_descriptions.append(desc)
            
            # CatÃ©goriser les anomalies
            categories = {}
            for desc in anomaly_descriptions:
                category = self.categorize_anomalie(desc)
                if category:
                    categories[category] = categories.get(category, 0) + 1
            
            # Analyser les prioritÃ©s d'urgence
            urgence_priorities = []
            for desc in urgence_descriptions:
                priority = self.prioritize_urgence(desc)
                urgence_priorities.append(priority['priority'])
            
            # GÃ©nÃ©rer des recommandations
            recommendations = []
            
            if categories:
                most_common = max(categories.items(), key=lambda x: x[1])
                recommendations.append(f"CatÃ©gorie la plus frÃ©quente : {most_common[0]} ({most_common[1]} incidents)")
            
            if urgence_priorities:
                critical_count = urgence_priorities.count('critical')
                if critical_count > 0:
                    recommendations.append(f"âš ï¸ {critical_count} urgence(s) critique(s) dÃ©tectÃ©e(s)")
            
            return {
                "period": f"{days_back} derniers jours",
                "total_anomalies": len(anomaly_descriptions),
                "total_urgences": len(urgence_descriptions),
                "categories_distribution": categories,
                "urgence_priorities": urgence_priorities,
                "recommendations": recommendations
            }
            
        except Exception as e:
            logging.error(f"âŒ Erreur analyse tendances: {e}")
            return {"error": str(e)}

# Instance globale
ai_assistant = ENCOAIAssistant()

# Fonctions utilitaires pour faciliter l'utilisation
def analyze_similar_anomalies(description: str) -> List[Dict]:
    """Raccourci pour analyser les anomalies similaires"""
    return ai_assistant.analyze_anomalie_similarity(description)

def suggest_resolution(description: str) -> Optional[str]:
    """Raccourci pour suggÃ©rer une rÃ©solution"""
    return ai_assistant.suggest_anomalie_resolution(description)

def get_operator_help(question: str, context: Dict = None) -> Optional[str]:
    """Raccourci pour l'assistance opÃ©rateur"""
    return ai_assistant.get_operator_assistance(question, context)

def analyze_system_trends(days: int = 30) -> Dict[str, Any]:
    """Raccourci pour l'analyse des tendances"""
    return ai_assistant.analyze_trends(days) 